---
permalink: /
title: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

Hi, I am a forth year undergraduate student at the [School of Statistics](http://stat.ruc.edu.cn/Home/index.htm), [Renmin University of China](https://en.ruc.edu.cn/). And I will pursue a two-year master's program at Renmin University of China.

Currently, I am working as a remote intern at the [Provable Responsible AI and Data Analytics (PRADA) Lab](https://www.kaust.edu.sa/en) at [KAUST](https://pradalab1.github.io/), advised by [Prof. Di Wang](https://shao3wangdi.github.io/).

Also, I am collaborating with the [TEA Lab](https://tea.ece.ubc.ca/) at the [University of British Columbia (UBC)](https://www.ubc.ca/).

Research Interests
======


My general research interest lies in **Trustworthy AI**. More specifically, I am currently focusing on the following directions:

- **Privacy in Reinforcement Learning with Human Feedback (RLHF):**  
  Exploring how to protect user/annotator privacy in the training process of large language models guided by human feedback.

- **Benign Overfitting in Large Language Models (LLMs):**  
  Studying the phenomenon where overparameterized models generalize well despite fitting noisy or mislabeled data.  
  This includes analyzing the **learning dynamics** of LLMs to better understand how and why benign overfitting occurs.

- **Data Valuation/Attribution:**  
  Developing efficient methods for estimating the value and influence of individual training data points, with applications to data pruning and selection.

I am broadly interested in making machine learning systems more **transparent**, **reliable**, and **accountable**.


News
======

- **Jul 8, 2025:**
  One paper have been accepted at the [Conference on Language Modeling (CoLM 2025)](https://colmweb.org/)!

- **Jun 27, 2025:**
  ðŸŽ“ I graduated with a Bachelor's degree from **Renmin University of China (RUC)**.

- **Feb 26, 2025:**
  We released our new paper ["Towards User-level Private Reinforcement Learning with Human Feedback"](https://arxiv.org/pdf/2502.17515)

  
Publications
======
\* indicates co-first authors.
  
- Towards User-level Private Reinforcement Learning with Human Feedback [[arxiv]](https://arxiv.org/pdf/2502.17515)


  **Jiaming Zhang**\*, Mingxi Lei\*, Meng Ding, Mengdi Li, Zihang Xiang, Difei Xu, Jinhui Xu, Di Wang


   Conference on Language Modeling (CoLM 2025)




Teaching
======
- Teaching Assistant of Regression Analysis, 2024 fall, Renmin University of China
